{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Notebook\n",
    "\n",
    "## DS 5110 Final Project \n",
    "\n",
    "## Air Quality Group \n",
    "\n",
    "* Daniel Heffley (dh3by)\n",
    "* Camille Leonard (cvl7qu)\n",
    "* Steph Verbout (sv8jy)\n",
    "* Shahriar Shahrokhabadi (ss3qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data \n",
    "\n",
    "Our original data sources were the [EPA's Air Quality Data](https://aqs.epa.gov/aqsweb/airdata/download_files.html#Raw) and [Socio-Economic data from OpenIntro](https://www.openintro.org/data/?data=county_complete).\n",
    "\n",
    "We compiled daily data for gas and particulate matter from 2017-2019 and then joined the air quality data with the Scoio-Economic data by county. The Socio-Economic data was published in 2010. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os \n",
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType, TimestampType, DecimalType\n",
    "from pyspark.sql.functions import col, asc, to_date, unix_timestamp, to_timestamp, count, when, isnan\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, Imputer, StringIndexer\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "import pyspark.sql.functions as f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/project/ds5559/Air_Quality_Group'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(r'/project/ds5559/Air_Quality_Group')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Create Final Dataset\") \\\n",
    "        .config(\"spark.executor.memory\", '20g') \\\n",
    "        .config('spark.executor.cores', '5') \\\n",
    "        .config('spark.executor.instances', '17') \\\n",
    "        .config(\"spark.driver.memory\",'1g') \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For viewing in the UI to monitor memory usage. Must be used from the desktop instance. \n",
    "# spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df_raw = spark.read.csv('DF_FINAL_AQI_W_INCOME.csv',inferSchema = True ,  header = True) #,schema = schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the schema is correct \n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns \n",
    "df_raw = df_raw.drop('_c0').drop('state + county')\n",
    "\n",
    "#Typecast date local\n",
    "df_raw = df_raw.withColumn('Date Local', df_raw['Date Local'].cast(DateType()))\n",
    "\n",
    "# Droping first and second group of features from DB\n",
    "CL1 = ['Site Num','Parameter Code_CO','Parameter Name_CO','Parameter Code_SO2','Parameter Name_SO2','Parameter Code_NO2']\n",
    "CL2 = ['Parameter Name_NO2','Parameter Code_O3','Parameter Name_O3','Parameter Code_PM2_5FRM','Parameter Name_PM2_5FRM','Parameter Code_PM2_5NON_FRM']\n",
    "CL3 = ['Parameter Name_PM2_5NON_FRM','Parameter Code_PM10MASS','Parameter Name_PM10MASS','Parameter Code_PM10SPEC','Parameter Name_PM10SPEC']\n",
    "CL4 = ['Parameter Code_HAPS','Parameter Name_HAPS','Parameter Code_LEAD','Parameter Name_LEAD','Parameter Code_NO','Parameter Name_NO']\n",
    "CL5 = ['Parameter Code_PRESS','Parameter Name_PRESS','Parameter Code_RH','Parameter Name_RH','Parameter Code_TEMP','Parameter Name_TEMP']\n",
    "CL6 = ['Parameter Code_WIND','Parameter Name_WIND']\n",
    "\n",
    "df_raw = df_raw.drop(*CL1,*CL2,*CL3,*CL4,*CL5,*CL6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping third group of features from DB\n",
    "CL7 = ['Event Type_CO','Event Type_SO2','Event Type_NO2','Event Type_O3','Event Type_PM2_5FRM','Event Type_PM2_5NON_FRM','Event Type_PM10MASS','Event Type_PM10SPEC']\n",
    "CL8 = ['AQI_NO2','AQI_O3','AQI_PM2_5FRM','AQI_PM2_5NON_FRM','AQI_PM10MASS']\n",
    "df_raw = df_raw.drop(*CL7,*CL8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only wind measurements that are Knots\n",
    "#df_raw = df_raw.filter(df_raw['Units of Measure_WIND'] == 'Knots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping fourth group of features from DB\n",
    "CL9 = ['Units of Measure_CO','Units of Measure_SO2','Units of Measure_NO2','Units of Measure_O3','Units of Measure_PM2_5FRM']\n",
    "CL10 = ['Units of Measure_PM2_5NON_FRM','Units of Measure_PM10MASS','Units of Measure_PM10SPEC','Units of Measure_HAPS','Units of Measure_LEAD']\n",
    "CL11 = ['Units of Measure_NO','Units of Measure_PRESS','Units of Measure_RH','Units of Measure_TEMP','Units of Measure_WIND']\n",
    "\n",
    "df_raw = df_raw.drop(*CL9,*CL10,*CL11)\n",
    "df_raw = df_raw.drop(*CL9,*CL10,*CL11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicate Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping duplicate data from dataset\n",
    "df_raw = df_raw.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Which Columns Have Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm no null values in data set \n",
    "df_raw.select(*[(count(c) / count(\"*\")).alias(c) for c in df_raw.columns]).show(vertical=True)\n",
    "# COUNT(*) is equivalent to COUNT(1) so NULLs won't be an issue\n",
    "#https://stackoverflow.com/questions/33900726/count-number-of-non-nan-entries-in-each-column-of-spark-dataframe-with-pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Missing Values - Fill with Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping incomplete rows from DB\n",
    "#df = df.na.drop(\"any\")\n",
    "\n",
    "# we need to impute data values \n",
    "# example \n",
    "\n",
    "imputeColumnList = ['Arithmetic Mean_HAPS', 'Arithmetic Mean_LEAD', 'Arithmetic Mean_NO', 'Arithmetic Mean_PRESS', 'Arithmetic Mean_RH', 'Arithmetic Mean_TEMP', 'Arithmetic Mean_WIND', 'Arithmetic Mean_PM2_5FRM',  'Arithmetic Mean_PM2_5NON_FRM','Arithmetic Mean_PM10MASS',  'Arithmetic Mean_PM10SPEC', 'pop_density', 'median_income']\n",
    "stringColumnList = ['Units of Measure_PM2_5FRM','Units of Measure_PM2_5NON_FRM','Units of Measure_PM10MASS','Units of Measure_PM10SPEC']\n",
    "\n",
    "imputer = Imputer(strategy='median',\n",
    "    inputCols=imputeColumnList, \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in imputeColumnList]\n",
    ")\n",
    "\n",
    "# Save imputed output as new data frame \n",
    "df = imputer.fit(df_raw).transform(df_raw)\n",
    "\n",
    "# Remove columns with null that were imputed \n",
    "df = df.drop('Arithmetic Mean_HAPS').drop('Arithmetic Mean_LEAD').drop('Arithmetic Mean_NO').drop('Arithmetic Mean_PRESS').drop('Arithmetic Mean_RH').drop('Arithmetic Mean_TEMP')\\\n",
    "       .drop('Arithmetic Mean_WIND').drop('Arithmetic Mean_PM2_5FRM').drop('Arithmetic Mean_PM2_5NON_FRM').drop('Arithmetic Mean_PM10SPEC').drop('Arithmetic Mean_PM10MASS')\\\n",
    "       .drop('pop_density').drop('median_income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm no null values in data set \n",
    "df.select(*[(count(c) / count(\"*\")).alias(c) for c in df.columns]).show(vertical=True)\n",
    "# COUNT(*) is equivalent to COUNT(1) so NULLs won't be an issue\n",
    "#https://stackoverflow.com/questions/33900726/count-number-of-non-nan-entries-in-each-column-of-spark-dataframe-with-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a record to confirm no processing errors \n",
    "#df.sample(0.000001, seed = 314).show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the dimensions of the data frame \n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rows with Missing AQI and Category Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm no null values in data set \n",
    "df.select(*[(count(c) / count(\"*\")).alias(c) for c in df.columns]).show(vertical=True)\n",
    "# COUNT(*) is equivalent to COUNT(1) so NULLs won't be an issue\n",
    "#https://stackoverflow.com/questions/33900726/count-number-of-non-nan-entries-in-each-column-of-spark-dataframe-with-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the dimensions of the data frame \n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL12 =['Date Local','State Code','County Code','State Name','County Name','Category','fips']\n",
    "df_out = df.drop(*CL12)\n",
    "\n",
    "bounds = {\n",
    "    c: dict(\n",
    "        zip([\"q1\", \"q3\"], df_out.approxQuantile(c, [0.25, 0.75], 0))\n",
    "    )\n",
    "    for c in df_out.columns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in bounds:\n",
    "    iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "    bounds[c]['lower'] = bounds[c]['q1'] - (iqr * 1.5)\n",
    "    bounds[c]['upper'] = bounds[c]['q3'] + (iqr * 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A = df.select('*').where(*[(df[c]<bounds[c]['lower']) | (df[c]>bounds[c]['upper']) for c in df_out.columns])\n",
    "\n",
    "df = df.filter(((df['Arithmetic Mean_CO']>bounds['Arithmetic Mean_CO']['lower']) | (df['Arithmetic Mean_CO']<bounds['Arithmetic Mean_CO']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_SO2']>bounds['Arithmetic Mean_SO2']['lower']) | (df['Arithmetic Mean_SO2']<bounds['Arithmetic Mean_SO2']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_NO2']>bounds['Arithmetic Mean_NO2']['lower']) | (df['Arithmetic Mean_NO2']<bounds['Arithmetic Mean_NO2']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_O3']>bounds['Arithmetic Mean_O3']['lower']) | (df['Arithmetic Mean_O3']<bounds['Arithmetic Mean_O3']['upper'])) &\\\n",
    "          ((df['AQI']>bounds['AQI']['lower']) | (df['AQI']<bounds['AQI']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_PM10SPEC_imputed']>bounds['Arithmetic Mean_PM10SPEC_imputed']['lower']) | (df['Arithmetic Mean_PM10SPEC_imputed']<bounds['Arithmetic Mean_PM10SPEC_imputed']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_LEAD_imputed']>bounds['Arithmetic Mean_LEAD_imputed']['lower']) | (df['Arithmetic Mean_LEAD_imputed']<bounds['Arithmetic Mean_LEAD_imputed']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_HAPS_imputed']>bounds['Arithmetic Mean_HAPS_imputed']['lower']) | (df['Arithmetic Mean_HAPS_imputed']<bounds['Arithmetic Mean_HAPS_imputed']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_NO_imputed']>bounds['Arithmetic Mean_NO_imputed']['lower']) | (df['Arithmetic Mean_NO_imputed']<bounds['Arithmetic Mean_NO_imputed']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_TEMP_imputed']>bounds['Arithmetic Mean_TEMP_imputed']['lower']) | (df['Arithmetic Mean_TEMP_imputed']<bounds['Arithmetic Mean_TEMP_imputed']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_PM2_5NON_FRM_imputed']>bounds['Arithmetic Mean_PM2_5NON_FRM_imputed']['lower']) | (df['Arithmetic Mean_PM2_5NON_FRM_imputed']<bounds['Arithmetic Mean_PM2_5NON_FRM_imputed']['upper'])) &\\\n",
    "          ((df['pop_density_imputed']>bounds['pop_density_imputed']['lower']) | (df['pop_density_imputed']<bounds['pop_density_imputed']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_PM2_5FRM_imputed']>bounds['Arithmetic Mean_PM2_5FRM_imputed']['lower']) | (df['Arithmetic Mean_PM2_5FRM_imputed']<bounds['Arithmetic Mean_PM2_5FRM_imputed']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_WIND_imputed']>bounds['Arithmetic Mean_WIND_imputed']['lower']) | (df['Arithmetic Mean_WIND_imputed']<bounds['Arithmetic Mean_WIND_imputed']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_RH_imputed']>bounds['Arithmetic Mean_RH_imputed']['lower']) | (df['Arithmetic Mean_RH_imputed']<bounds['Arithmetic Mean_RH_imputed']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_PRESS_imputed']>bounds['Arithmetic Mean_PRESS_imputed']['lower']) | (df['Arithmetic Mean_PRESS_imputed']<bounds['Arithmetic Mean_PRESS_imputed']['upper'])) &\\\n",
    "          ((df['Arithmetic Mean_PM10MASS_imputed']>bounds['Arithmetic Mean_PM10MASS_imputed']['lower']) | (df['Arithmetic Mean_PM10MASS_imputed']<bounds['Arithmetic Mean_PM10MASS_imputed']['upper'])) &\\\n",
    "          ((df['median_income_imputed']>bounds['median_income_imputed']['lower']) | (df['median_income_imputed']<bounds['median_income_imputed']['upper'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------\n",
      " Date Local                               | 2018-03-21         \n",
      " State Code                               | 29                 \n",
      " County Code                              | 510                \n",
      " State Name                               | Missouri           \n",
      " County Name                              | St. Louis City     \n",
      " Arithmetic Mean_CO                       | 0.247625           \n",
      " Arithmetic Mean_SO2                      | 0.036364           \n",
      " Arithmetic Mean_NO2                      | 10.245833          \n",
      " Arithmetic Mean_O3                       | 0.029824           \n",
      " AQI                                      | 43.0               \n",
      " Category                                 | Good               \n",
      " fips                                     | 29510              \n",
      " Arithmetic Mean_PM10SPEC_imputed         | 1.0                \n",
      " Arithmetic Mean_LEAD_imputed             | 0.0032             \n",
      " Arithmetic Mean_HAPS_imputed             | 0.0                \n",
      " Arithmetic Mean_NO_imputed               | 1.4125             \n",
      " Arithmetic Mean_TEMP_imputed             | 42.25              \n",
      " Arithmetic Mean_PM2_5NON_FRM_imputed     | 11.6               \n",
      " pop_density_imputed                      | 5157.5             \n",
      " Arithmetic Mean_PM2_5FRM_imputed         | 10.3               \n",
      " Arithmetic Mean_WIND_imputed             | 312.166667         \n",
      " Arithmetic Mean_RH_imputed               | 54.041667000000004 \n",
      " Arithmetic Mean_PRESS_imputed            | 997.508333         \n",
      " Arithmetic Mean_PM10MASS_imputed         | 27.0               \n",
      " median_income_imputed                    | 28186.0            \n",
      " Arithmetic Mean_CO_out                   | 0                  \n",
      " Arithmetic Mean_SO2_out                  | 0                  \n",
      " Arithmetic Mean_NO2_out                  | 0                  \n",
      " Arithmetic Mean_O3_out                   | 0                  \n",
      " AQI_out                                  | 0                  \n",
      " Arithmetic Mean_PM10SPEC_imputed_out     | 0                  \n",
      " Arithmetic Mean_LEAD_imputed_out         | 0                  \n",
      " Arithmetic Mean_HAPS_imputed_out         | 0                  \n",
      " Arithmetic Mean_NO_imputed_out           | 0                  \n",
      " Arithmetic Mean_TEMP_imputed_out         | 0                  \n",
      " Arithmetic Mean_PM2_5NON_FRM_imputed_out | 0                  \n",
      " pop_density_imputed_out                  | 0                  \n",
      " Arithmetic Mean_PM2_5FRM_imputed_out     | 0                  \n",
      " Arithmetic Mean_WIND_imputed_out         | 0                  \n",
      " Arithmetic Mean_RH_imputed_out           | 0                  \n",
      " Arithmetic Mean_PRESS_imputed_out        | 0                  \n",
      " Arithmetic Mean_PM10MASS_imputed_out     | 0                  \n",
      " median_income_imputed_out                | 0                  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df.select(\n",
    "    \"*\",\n",
    "    *[\n",
    "        f.when(\n",
    "            f.col(c).between(bounds[c]['lower'], bounds[c]['upper']),\n",
    "            0\n",
    "        ).otherwise(1).alias(c+\"_out\") \n",
    "        for c in df_out.columns\n",
    "    ]\n",
    ").show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv file  ####CHECKPOINT DATASET#####\r\n",
    "df.repartition(1).write.format('com.databricks.spark.csv').save('df_cleaned.csv',header = 'true')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27c61da0a00253259337c45142adbf2ab8dba76584f9ba34a39b507e840c11eb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}